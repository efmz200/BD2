Erick Madrigal 2018146983
# Apache Spark: Un motor unificado para el procesamiento de macrodatos
Spark es una herramienta para el procesamiento de grandes cantidades de datos que extiende el modelo de programación MapReduce con una abstracción
llamada Resilient Distributed Datasets (RDDs). Esto le permite capturar una amplia gama de cargas de trabajo de procesamiento, incluyendo SQL, 
streaming, machine learning y procesamiento de grafos.

La generalidad de Spark tiene varios beneficios importantes. Primero, las aplicaciones son más fáciles de desarrollar porque usan una API unificada.
En segundo lugar, es más eficiente combinar tareas de procesamiento; mientras que los sistemas anteriores requerían escribir los datos en almacenamiento
para pasarlos a otro motor, Spark puede ejecutar diversas funciones sobre los mismos datos, a menudo en memoria. Por último, Spark permite nuevas 
aplicaciones que no eran posibles con sistemas anteriores.

# Modelo de programación
La abstracción de programación clave en Spark son los RDD, que son colecciones tolerantes a fallos de objetos particionados en un clúster que pueden 
manipularse en paralelo. Los usuarios crean RDD aplicando operaciones denominadas "transformaciones". Spark expone los RDD a través de una API de 
programación funcional en Scala, Java, Python y R, donde los usuarios pueden simplemente pasar funciones locales para ejecutarlas en el clúster.

Spark evalúa los RDD de forma perezosa, lo que le permite encontrar un plan eficiente para el cálculo del usuario. Cuando se llama a una acción, 
Spark examina todo el grafo de transformaciones utilizadas para crear un plan de ejecución. Por último, los RDD ofrecen soporte explícito para
compartir datos entre cálculos. Por defecto, los RDD son "efímeros", es decir, se vuelven a calcular cada vez que se utilizan en una acción.

Spark es un sistema de computación distribuida que proporciona tolerancia a fallos mediante un enfoque llamado "lineage". Cada RDD realiza un 
seguimiento de la cadena de transformaciones utilizadas para construirlo y las vuelve a ejecutar en los datos base para reconstruir cualquier 
partición perdida.

Spark se integra con sistemas de almacenamiento como HDFS y bases de datos como S3 y Cassandra. Además, Spark cuenta con bibliotecas de alto 
nivel para procesamiento de datos, incluyendo Spark SQL, Spark Streaming, GraphX y MLlib.

Spark SQL implementa consultas relacionales en Spark mediante técnicas similares a las bases de datos analíticas. Spark Streaming procesa 
datos en tiempo real mediante la división de los datos en lotes pequeños y la combinación de resultados mediante RDDs. GraphX proporciona 
una interfaz de cómputo de gráficos similar a Pregel y GraphLab, mientras que MLlib implementa más de 50 algoritmos comunes para el entrenamiento 
de modelos distribuidos.

Las bibliotecas de Spark operan sobre RDDs como la abstracción de datos, lo que las hace fáciles de combinar en aplicaciones. Además,
Spark implementa optimizaciones para igualar el rendimiento de motores especializados.

# Aplicaciones 
Spark se utiliza para procesamiento en lotes, consultas interactivas, procesamiento de datos en tiempo real y aplicaciones científicas. 
Los casos de uso incluyen la conversión de datos de formato crudo a un formato estructurado, entrenamiento de modelos de aprendizaje automático,
consultas relacionales, procesamiento de imágenes y datos genómicos, detección de spam y toma de decisiones en tiempo real.

Spark se integra con diferentes entornos de implementación, no solo con Hadoop. Spark Streaming se utiliza en el 46% de las organizaciones y 
el aprendizaje automático en el 54%. Spark se utiliza en aplicaciones interactivas, científicas y empresariales, y las bibliotecas de Spark 
facilitan la combinación de casos de uso en aplicaciones.

# Modelo general de Spark
Perspectiva de expresividad. MapReduce puede emular cualquier cómputo distribuido al ofrecer las operaciones map y reduce. Sin embargo, 
es ineficiente en compartir datos a través del tiempo y fue diseñado para entornos de lotes con minutos u horas de latencia. RDDs y Spark 
abordan estas limitaciones al evitar la replicación de datos intermedios y permitir el intercambio rápido de datos a través del tiempo.

Perspectiva de sistemas. Las aplicaciones en clúster están limitadas por las propiedades del hardware subyacente, como la jerarquía de 
almacenamiento y la limitación de CPU. La colocación de datos y cómputo en la red es crucial para el rendimiento de muchas aplicaciones. 
Aunque Spark puede ser más costoso que otros sistemas especializados debido a su tolerancia a fallos, su rendimiento es competitivo.






